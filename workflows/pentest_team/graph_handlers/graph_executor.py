from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver

from utils.common_utils import save_and_open_graph, generate_unique_id
from utils.langraph import mapper
from utils.langraph.mapper import save_snapshot_in_json

from typing import Dict, Any, Optional


def launch_as_subgraph(compiled_graph, inputs: Dict[str, Any], thread_id: int) -> Any:
    """
    Launch a subgraph using the provided compiled graph and inputs.

    Args: compiled_graph: The compiled workflow to be executed as a subgraph. inputs (Dict[str, Any]): A dictionary
    containing the input data (e.g., messages, sender) for the subgraph execution. thread_id (int): The thread
    identifier used for configuring the execution.

    Returns:
        Any: The last event from the execution if successful, or an error message if an error occurs during execution.
    """
    # Validate inputs
    if inputs is None:
        return "Error: No input provided. Please supply valid inputs to execute the graph."

    # Set up execution configuration with the provided thread ID
    config = {"configurable": {"thread_id": thread_id}}

    # Initialize a variable to store the last event of execution results
    event: Optional[Dict[str, Any]] = None

    try:
        # Set to store the IDs of already processed messages
        processed_message_ids = set()

        # Stream results from the graph execution and print the output
        for event in compiled_graph.stream(inputs, config, stream_mode='values'):
            # Loop through all the messages in the event
            for message in event['messages']:
                # Try to get the message ID
                message_id = getattr(message, 'id', None)

                # If there's no ID, generate a unique one
                if message_id is None:
                    message_id = generate_unique_id(message.content)
                    message.id = message_id

                # Check if the message has already been processed
                if message_id not in processed_message_ids:
                    # Print the message
                    message.pretty_print()
                    # Add the message ID to the set of processed ones
                    processed_message_ids.add(message_id)

    except Exception as e:
        # Return an error message in case of an exception
        return f"Execution error: {str(e)}"

    # Return the last event or an error message if no data was processed
    return event if event is not None else "Error: No data processed from the workflow."


def launch_as_standalone_agent(graph, input_message: str, team_name: str):
    """
    Launch a standalone agent by compiling the provided graph and executing the workflow.

    This method compiles the provided graph using a memory-based checkpoint system, streams
    the output messages, and executes the agent's tasks based on a human-provided input message.
    It also saves the state snapshots for potential future recovery or analysis.

    Args:
        graph: The graph (workflow) to be compiled and executed as a standalone agent.
        input_message (str): The initial input message provided by the human, which is used to
                             generate a pentest plan or execute other tasks.
        team_name (str): The name of the team or agent, used for saving state snapshots.

    The function performs the following steps:
    - Compiles the provided graph with a memory-based checkpoint for recovery.
    - Supplies the initial inputs (a human message) to the graph.
    - Streams and prints the output messages generated by the agent during execution.
    - Saves the state snapshots to a JSON file for future use.
    """
    memory = MemorySaver()

    # Compile the graph with a memory-based checkpoint
    compiled_graph = graph.compile(checkpointer=memory)

    # Save and optionally open the compiled graph for further inspection
    save_and_open_graph(compiled_graph)

    # Execution configuration for the graph
    config = {"configurable": {"thread_id": 2}}

    # Initial input containing a human message (task description)
    inputs = {
        "messages": [
            HumanMessage(
                content=input_message
            )
        ],
        'sender': 'human'
    }

    # Set to store the IDs of already processed messages
    processed_message_ids = set()

    # Stream results from the graph execution and print the output
    for event in compiled_graph.stream(inputs, config, stream_mode='values'):
        # Loop through all the messages in the event
        for message in event['messages']:
            # Try to get the message ID
            message_id = getattr(message, 'id', None)

            # If there's no ID, generate a unique one
            if message_id is None:
                message_id = generate_unique_id(message.content)
                message.id = message_id

            # Check if the message has already been processed
            if message_id not in processed_message_ids:
                # Print the message
                message.pretty_print()
                # Add the message ID to the set of processed ones
                processed_message_ids.add(message_id)

    # Save the states in a file
    states_list = list(compiled_graph.get_state_history(config=config))
    save_snapshot_in_json(
        states_list,
        team_name=team_name
    )



def launch_graph_as_host(
        graph,
        task_message: Optional[str] = None,
        live_mode: bool = False,
        file_path: Optional[str] = None
):
    """
    Launch the compiled workflow graph as a host investigation agent, either in live mode
    or by resuming from a saved state snapshot.

    This method compiles the provided workflow graph, optionally loads a previously saved state,
    and continues executing the workflow with updated state information. In live mode,
    the agent processes a real-time task or investigation based on the provided task message.

    Args:
        graph: The workflow to be compiled and executed as a host investigation agent.
        task_message (Optional[str]): A string containing the task description or specific instructions
                                      for the agent. Used in live mode.
        live_mode (bool): Indicates whether to run the workflow in live mode (real-time task
                          execution) or resume from a saved state.
        file_path (Optional[str]): The path to the saved state JSON file, used when resuming execution.

    The function will:
    - Compile the workflow graph using a memory-based checkpoint system.
    - Optionally run a real-time task with the `task_message` if `live_mode` is True.
    - If `live_mode` is False, load a previously saved state from `file_path` and continue execution from that point.
    - Stream and print the output messages from the graph's execution.
    """
    memory = MemorySaver()

    # Compile the graph with a memory-based checkpoint
    compiled_graph = graph.compile(checkpointer=memory)

    # Save and optionally open the compiled graph for further processing or inspection
    save_and_open_graph(compiled_graph)

    if live_mode and task_message:
        # Configuration for the execution of the graph in live mode
        config = {"configurable": {"thread_id": 1}}

        # Initial input containing a human message with task or investigation instructions
        inputs = {
            "messages": [
                HumanMessage(
                    content=task_message  # Use task_message to define the task for investigation
                )
            ],
            'sender': 'human'
        }

        # Stream results in real-time based on the input task message
        try:
            processed_message_ids = set()
            for event in compiled_graph.stream(inputs, config, stream_mode='values'):
                for message in event['messages']:
                    message_id = getattr(message, 'id', None)
                    if message_id is None:
                        message_id = generate_unique_id(message.content)
                        message.id = message_id
                    if message_id not in processed_message_ids:
                        message.pretty_print()
                        processed_message_ids.add(message_id)
        except Exception as e:
            print(f"Execution error: {str(e)}")

    elif not live_mode and file_path:
        # Load a previously saved state snapshot from the provided file path
        state_snapshot = mapper.load_snapshot_from_json(file_path=file_path)

        # Update the graph's state with the loaded snapshot
        updating_config = compiled_graph.update_state(state_snapshot.config, state_snapshot.values)

        # Stream and print the output messages from the graph execution, resuming from the saved state
        try:
            processed_message_ids = set()
            for event in compiled_graph.stream(None, updating_config, stream_mode='values'):
                for message in event['messages']:
                    message_id = getattr(message, 'id', None)
                    if message_id is None:
                        message_id = generate_unique_id(message.content)
                        message.id = message_id
                    if message_id not in processed_message_ids:
                        message.pretty_print()
                        processed_message_ids.add(message_id)
        except Exception as e:
            print(f"Execution error: {str(e)}")

    else:
        raise ValueError("Either 'live_mode' must be True with a 'task_message', or 'file_path' must be provided for "
                         "resuming.")
